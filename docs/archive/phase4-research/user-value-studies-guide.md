# User Value Studies - Research Guide
**Epic 4: Proof of Value - Deliverable 3**
**UX Researcher: Phase 4 Product Strategy**
**Date:** December 31, 2025
**Status:** Ready for Execution

---

## Study Overview

### Objective
Conduct 20+ in-depth user interviews to understand **perceived value, adoption barriers, and improvement opportunities** for Skillsmith skills.

### Research Questions

1. **Value Perception**
   - What value do users attribute to skills?
   - How do users calculate ROI (implicitly or explicitly)?
   - Which value dimensions matter most? (time, quality, satisfaction)

2. **Adoption Patterns**
   - What triggers skill discovery and installation?
   - What barriers prevent adoption?
   - How do users decide which skills to try?

3. **Usage Context**
   - When do users choose skills vs. manual workflows?
   - What contextual factors influence usage?
   - How do skills integrate into existing workflows?

4. **Trust & Credibility**
   - What builds trust in skill outputs?
   - What causes skepticism or abandonment?
   - How do users verify skill quality?

5. **Improvement Opportunities**
   - What frustrations exist with current skills?
   - What features would increase value?
   - What new skills would users create if they could?

---

## Participant Recruitment

### Sample Composition

**Target:** 24 participants (6 per persona, 4 as buffer)

| Persona | n | Recruitment Criteria |
|---------|---|---------------------|
| **Solo Developer** | 6 | Hobby/side projects, <5 team members, any experience level |
| **Professional Developer** | 6 | Enterprise team (5-50 people), 2-10 yrs experience |
| **DevOps Engineer** | 6 | Infrastructure focus, CI/CD responsibilities |
| **Engineering Manager** | 6 | Team leadership, budget authority, technical background |

**Diversity Requirements:**
- Gender: ≥40% underrepresented genders
- Geography: ≥30% non-North America
- Company size: Mix of startup (<50), midsize (50-500), enterprise (500+)
- Skillsmith experience: Mix of new (<1 week), active (1-4 weeks), power users (>4 weeks)

### Recruitment Channels

1. **Skillsmith User Base** (if available)
   - Email invitation to random sample
   - In-product recruitment banner (opt-in)

2. **Developer Communities**
   - Reddit: r/webdev, r/devops, r/ExperiencedDevs
   - Twitter/X: Claude Code hashtags
   - Discord: Claude AI server, developer servers
   - LinkedIn: Developer groups

3. **Professional Networks**
   - Personal connections (via team members)
   - Conference attendees (past speaker lists)
   - GitHub active contributors

### Screening Survey

**Duration:** 3 minutes
**Completion incentive:** Entry into $100 gift card raffle

**Questions:**

1. **Demographics**
   - Role: [ ] Developer [ ] DevOps [ ] Manager [ ] Other: _____
   - Years of experience: _____
   - Team size: [ ] Solo [ ] 2-5 [ ] 6-20 [ ] 21-50 [ ] 50+
   - Primary language/framework: _____

2. **Skillsmith Usage**
   - Have you used Claude Code? [ ] Yes [ ] No
   - Have you heard of Skillsmith? [ ] Yes [ ] No
   - If yes, how many skills installed? [ ] 0 [ ] 1-3 [ ] 4-7 [ ] 8+
   - How long using Skillsmith? [ ] <1 week [ ] 1-4 weeks [ ] 1-3 months [ ] 3+ months

3. **Screening Criteria**
   - Are you comfortable discussing your development workflow? [ ] Yes [ ] No
   - Can you share screen during a video call? [ ] Yes [ ] No
   - Can you commit to 45 minutes for an interview? [ ] Yes [ ] No

4. **Contact**
   - Email: _____
   - Preferred interview time: [ ] Morning [ ] Afternoon [ ] Evening
   - Time zone: _____

**Selection:**
- Prioritize users with 1-4 weeks Skillsmith experience (ideal recall)
- Balance personas evenly
- Exclude Skillsmith team members and close affiliates

---

## Interview Protocol

### Logistics

**Duration:** 45 minutes
**Format:** Video call (Zoom, Google Meet)
**Recording:** Yes (with consent)
**Compensation:** $50 USD gift card (Amazon, Visa, crypto)
**Facilitator:** UX Researcher or trained interviewer (not Skillsmith engineer)

### Pre-Interview (5 minutes before call)

**Facilitator checklist:**
- [ ] Review participant screening responses
- [ ] Test recording setup
- [ ] Prepare interview guide with participant name
- [ ] Have gift card code ready

**Participant email (24 hours before):**
```
Subject: Interview Reminder - Skillsmith Value Study Tomorrow

Hi [Name],

Looking forward to our interview tomorrow at [Time]!

Before we meet:
1. Please have 1-2 skills installed (if you don't yet)
2. Be ready to share your screen (we'll look at real examples)
3. No preparation needed - just bring your honest thoughts!

Join link: [Zoom URL]
Duration: 45 minutes
Compensation: $50 gift card sent within 48 hours

Questions? Reply to this email.

Best,
[Facilitator Name]
```

---

## Interview Guide

### Part 1: Introduction (5 minutes)

**Facilitator script:**

> "Thanks for joining! I'm [Name], a researcher studying how developers experience Skillsmith. This isn't a test of you - we want to understand what works, what doesn't, and how we can improve.
>
> I'll ask about your workflow, how you use skills, and what value they provide. There are no right or wrong answers - honest feedback helps us most.
>
> I'll record this session to review later. The recording is confidential and used only for research. Is that okay?
>
> [Wait for consent. If yes, start recording.]
>
> Great! Let's start with some background about you."

---

### Part 2: Background & Context (5 minutes)

**Questions:**

1. **"Tell me about your typical workday. What does a productive day look like?"**
   - *Probe:* What tools do you use most? What takes up most of your time?
   - *Listen for:* Workflow patterns, pain points, tool preferences

2. **"When did you first hear about Skillsmith? What made you try it?"**
   - *Probe:* What were you hoping it would do? Who recommended it?
   - *Listen for:* Discovery channels, initial expectations

3. **"Walk me through how you currently use skills, if at all."**
   - *Probe:* Which skills? How often? In what situations?
   - *Listen for:* Usage frequency, context, integration into workflow

---

### Part 3: Value Perception (15 minutes)

**Activity: Concrete Examples**

> "Let's look at specific examples. Can you share your screen and show me a recent time you used a skill?"

**Questions:**

4. **"Tell me the story of this example. What were you trying to do?"**
   - *Probe:* What would you have done without the skill? Why use it this time?
   - *Listen for:* Decision criteria, manual baseline comparison

5. **"What value did the skill provide here?"**
   - *Probe:* Time saved? Better quality? Less frustration? Something else?
   - *Listen for:* Value dimensions (time, quality, cognitive load, satisfaction)
   - *Note:* Capture exact quotes about value

6. **"How much time did this skill save you, roughly?"**
   - *Probe:* Seconds? Minutes? Hours? How often do you do this task?
   - *Calculate:* Time saved × frequency = aggregate value
   - *Listen for:* ROI perception, even if imprecise

7. **"Show me another example where a skill was especially valuable."**
   - *Repeat questions 4-6 for second example*
   - *Listen for:* Pattern across examples, consistency in value perception

8. **"Have you ever NOT used a skill when you could have? Tell me about that."**
   - *Probe:* Why skip it? What was the context? Did you regret it?
   - *Listen for:* Barriers to use, edge cases, failure modes

**Ranking Activity:**

9. **"If you could only keep 3 skills, which would they be? Why those three?"**
   - *Record:* 1st choice, 2nd choice, 3rd choice + rationales
   - *Listen for:* Relative value, core vs. nice-to-have

---

### Part 4: Trust & Quality (10 minutes)

**Questions:**

10. **"How do you know when a skill's output is good enough to use?"**
    - *Probe:* Do you review it? Modify it? Trust it completely?
    - *Listen for:* Verification behaviors, trust signals

11. **"Has a skill ever given you bad output? Tell me about that."**
    - *Probe:* What happened? How did you realize? Did you stop using it?
    - *Listen for:* Failure recovery, trust erosion

12. **"What makes you trust a skill? Or distrust it?"**
    - *Probe:* Official vs. community? Reviews? Author reputation?
    - *Listen for:* Trust dimensions (verification, community, tier)

13. **"If you were recommending Skillsmith to a colleague, what would you say?"**
    - *Listen for:* Value proposition in users' own words (use for marketing)
    - *Note:* This is an NPS proxy

---

### Part 5: Improvement Opportunities (8 minutes)

**Questions:**

14. **"What's frustrating about using skills right now?"**
    - *Probe:* Installation? Discovery? Configuration? Performance?
    - *Listen for:* Top friction points

15. **"If you could change ONE thing about Skillsmith, what would it be?"**
    - *Listen for:* Highest-priority improvement (direct backlog input)

16. **"What skill doesn't exist yet that you wish existed?"**
    - *Probe:* What would it do? What value would it provide?
    - *Listen for:* Unmet needs, new skill ideas

17. **"How much would you pay for Skillsmith, if it cost money?"**
    - *Probe:* Per month? One-time? Per skill? Why that amount?
    - *Listen for:* Willingness to pay (pricing research)
    - *Note:* Frame as hypothetical, not a pricing announcement

---

### Part 6: Wrap-Up (2 minutes)

**Questions:**

18. **"Is there anything else about your experience with skills that we didn't cover?"**
    - *Listen for:* Unexpected insights

19. **"Can we follow up in 2 months to see how your usage has evolved?"**
    - *Record:* Willingness for longitudinal study

**Facilitator closing:**

> "Thank you so much! Your insights are incredibly valuable. You'll receive your $50 gift card via email within 48 hours. If you have questions later, feel free to email me at [email].
>
> One last thing: can I share anonymized quotes from this interview in our research report? No names or identifying info - just insights like 'A senior developer said...'
>
> [If yes:] Great, thank you!
> [Stop recording]
> Have a great rest of your day!"

---

## Interview Tips for Facilitators

### Effective Probing Techniques

**When to probe deeper:**
- Vague answers ("It's good", "Saves time")
- Interesting hints ("Well, sometimes...")
- Contradictions ("I love it but never use it")

**Probe phrases:**
- "Tell me more about that..."
- "Can you give me a specific example?"
- "What do you mean by [their word]?"
- "Why did you do it that way?"
- "How did that make you feel?"

**Avoid:**
- Leading questions ("Don't you think skills are amazing?")
- Jargon that participant didn't use first
- Defensiveness if they criticize Skillsmith
- Interrupting stories - let them talk!

---

## Data Analysis Plan

### Interview Synthesis Process

**Phase 1: Transcription (Within 48 hours)**
- Use Otter.ai or similar for automated transcription
- Review and correct for accuracy
- Anonymize participant names → P1, P2, ..., P24

**Phase 2: Coding (Week 1-2)**
- Develop codebook from first 5 interviews
- Apply codes to all transcripts
- Track inter-rater reliability (2 coders, ≥80% agreement)

**Codebook structure:**
```
1. Value Dimensions
   1.1 Time savings
   1.2 Quality improvement
   1.3 Cognitive load reduction
   1.4 Learning ease
   1.5 Satisfaction

2. Barriers
   2.1 Discovery
   2.2 Installation
   2.3 Configuration
   2.4 Performance
   2.5 Trust

3. Usage Context
   3.1 Workflow integration
   3.2 Task types
   3.3 Frequency

4. Improvement Ideas
   4.1 Existing skill enhancement
   4.2 New skill requests
   4.3 Platform features
```

**Phase 3: Thematic Analysis (Week 2-3)**
- Group codes into themes
- Identify patterns across personas
- Extract representative quotes
- Count frequency of themes (quantify qualitative)

**Example analysis table:**
| Theme | Frequency | Personas | Representative Quote |
|-------|-----------|----------|---------------------|
| Time savings as primary value | 21/24 (88%) | All | "I used to spend 5 minutes on commit messages. Now it's 30 seconds." - P7 |
| Trust through verification | 18/24 (75%) | Prof Dev, Mgr | "I always review the output first. Can't just blindly trust AI." - P12 |
| Discovery is hard | 15/24 (63%) | Solo, DevOps | "I don't know what skills exist. I just use what's suggested." - P3 |

---

## Synthesis Report Structure

**Report format:** 20-30 page PDF + slide deck

### Section 1: Executive Summary (2 pages)

**Contents:**
- Study overview (n=24, 4 personas, 45-min interviews)
- Top 5 insights (bullet points)
- Implications for product strategy
- Recommended next steps

---

### Section 2: Methodology (2 pages)

**Contents:**
- Recruitment process
- Sample demographics (table)
- Interview protocol overview
- Analysis approach (coding, thematic analysis)
- Limitations

---

### Section 3: Key Findings (10 pages)

**For each research question, include:**

**3.1 Value Perception**
- Primary value dimensions identified
- How users calculate ROI
- Quotes illustrating value perception
- Persona differences in value prioritization

**3.2 Adoption Patterns**
- Triggers for skill discovery
- Barriers preventing adoption
- Decision criteria for trying skills
- Onboarding experience analysis

**3.3 Usage Context**
- When skills are used vs. manual workflows
- Contextual factors influencing usage
- Workflow integration patterns
- Frequency and consistency of use

**3.4 Trust & Credibility**
- What builds trust
- What causes skepticism
- Verification behaviors
- Failure recovery patterns

**3.5 Improvement Opportunities**
- Top frustrations (ranked by frequency)
- Feature requests
- New skill ideas
- Pricing insights

---

### Section 4: Updated Personas (4 pages)

**For each persona, update:**

**Before (Assumption-Based Persona):**
- Goals
- Pain points
- Behaviors

**After (Data-Driven Persona):**
- Validated goals (from interviews)
- Actual pain points (coded from transcripts)
- Observed behaviors (real usage patterns)
- Value expectations (what they care about)
- Quotes bringing persona to life

**Example:**

```markdown
## Persona: Professional Developer (Updated)

**Name:** Sarah (Composite of P4, P7, P11, P15, P19, P22)

**Goals (Validated):**
✓ Ship quality code faster [Confirmed - 6/6 mentioned]
✓ Reduce context switching [New insight - 5/6 mentioned]
✗ Learn new frameworks [Not mentioned - remove]

**Pain Points (Actual):**
1. "Code review takes forever" - All 6 participants
2. "Forgetting project context" - 5/6
3. "Writing boilerplate is tedious" - 4/6

**Value Perception:**
- Primary: Time savings on repetitive tasks
- Secondary: Code quality confidence
- Willing to pay: $20-50/month for team license

**Key Quote:**
> "I'd pay $30/month just for the commit and review-pr skills.
> They save me an hour a day, easy." - P7
```

---

### Section 5: Improvement Backlog (3 pages)

**Format:** Prioritized list with supporting data

| Priority | Improvement | Frequency | Estimated Impact | Effort |
|----------|-------------|-----------|------------------|--------|
| P0 | Improve skill discovery (in-product recommendations) | 15/24 (63%) | High | Medium |
| P0 | Add skill output verification UI | 18/24 (75%) | High | High |
| P1 | One-click skill installation | 12/24 (50%) | Medium | Low |
| P1 | Skill usage analytics dashboard | 9/24 (38%) | Medium | Medium |
| P2 | Skill marketplace with ratings | 8/24 (33%) | Low | High |

**For each item:**
- User need (quote from interviews)
- Proposed solution
- Success metrics
- Dependencies

---

### Section 6: Recommendations (2 pages)

**Strategic recommendations:**

1. **Product Strategy**
   - Focus on time savings (primary value driver)
   - Prioritize trust-building features (verification UI)
   - Invest in discovery (contextual recommendations)

2. **Design Priorities**
   - Simplify installation (one-click)
   - Surface value (analytics, attribution)
   - Build trust (output verification)

3. **Business Model**
   - Freemium viable ($20-50/month willingness to pay for team)
   - Bundle high-value skills (commit + review-pr)
   - Offer enterprise tier (centralized management)

4. **Research Next Steps**
   - Longitudinal study (track value over 3 months)
   - A/B test discovery improvements
   - Quantify time savings with instrumentation

---

## Persona Update Templates

### Template: Data-Driven Persona

```markdown
# [Persona Name] - Updated from User Research

**Based on:** [Number] interviews (P[IDs])
**Last updated:** [Date]

---

## Demographics (Actual)

| Attribute | Data |
|-----------|------|
| Role | [Most common role from interviews] |
| Experience | [Range from participants] |
| Team size | [Range from participants] |
| Company type | [Startup, midsize, enterprise - distribution] |

---

## Goals (Validated)

1. **[Goal 1]** - ✓ Confirmed by [X/N participants]
   - Quote: "[Representative quote]"

2. **[Goal 2]** - ✓ Confirmed by [X/N participants]
   - Quote: "[Representative quote]"

3. **[Goal 3]** - ✗ Not validated - Remove from persona

---

## Pain Points (Discovered)

1. **[Pain point 1]** - Mentioned by [X/N participants]
   - Context: [When this pain occurs]
   - Current workaround: [How they cope]
   - Quote: "[Representative quote]"

2. **[Pain point 2]** - Mentioned by [X/N participants]
   - Context: [When this pain occurs]
   - Current workaround: [How they cope]
   - Quote: "[Representative quote]"

---

## Workflow Patterns (Observed)

**Typical day:**
- [Activity 1]: [Time spent], [Tools used]
- [Activity 2]: [Time spent], [Tools used]
- [Activity 3]: [Time spent], [Tools used]

**Skillsmith usage:**
- Frequency: [Daily / Weekly / Monthly]
- Top skills: [Skill 1], [Skill 2], [Skill 3]
- Trigger: [What prompts usage]

---

## Value Perception

**Primary value dimension:** [Time / Quality / Satisfaction / etc.]

**ROI calculation:**
- Time saved per day: [X minutes]
- Task frequency: [X times per week]
- Annual value: [Calculated or estimated]

**Willingness to pay:** [Range from interviews]

**Key quote:**
> "[Most representative quote about value]" - P[ID]

---

## Trust & Decision Criteria

**What builds trust:**
- [Factor 1] - [X/N participants]
- [Factor 2] - [X/N participants]

**What causes skepticism:**
- [Factor 1] - [X/N participants]
- [Factor 2] - [X/N participants]

**Verification behaviors:**
- [Behavior 1]: [Frequency]
- [Behavior 2]: [Frequency]

---

## Improvement Priorities

1. **[Most requested improvement]** - [X/N participants]
2. **[Second most requested]** - [X/N participants]
3. **[Third most requested]** - [X/N participants]

---

## Design Implications

**For onboarding:**
- [Insight 1 from interviews]
- [Insight 2 from interviews]

**For feature prioritization:**
- [Insight 1 from interviews]
- [Insight 2 from interviews]

**For messaging:**
- Use language like: "[Quote showing how they describe value]"
- Avoid: "[Jargon or framing that confused participants]"

---

## Representative Participant Profiles

**P[ID]: [Brief description]**
- Most representative of this persona
- Quote: "[Why they're typical]"

**P[ID]: [Brief description]**
- Edge case or outlier
- Quote: "[What made them different]"
```

---

## Timeline & Deliverables

### Week 1-2: Recruitment
- [ ] Screen 50+ applicants
- [ ] Select 24 participants (4 per persona × 6)
- [ ] Schedule interviews
- [ ] Send confirmation emails

### Week 3-4: Interviews
- [ ] Conduct 24 interviews (6 per week)
- [ ] Send gift cards within 48 hours
- [ ] Begin transcription

### Week 5-6: Analysis
- [ ] Develop codebook
- [ ] Code all transcripts
- [ ] Identify themes
- [ ] Extract quotes

### Week 7: Synthesis
- [ ] Write synthesis report
- [ ] Update personas
- [ ] Create improvement backlog
- [ ] Design slide deck

### Week 8: Delivery
- [ ] Present findings to team
- [ ] Share report with stakeholders
- [ ] Archive recordings and transcripts (secure)
- [ ] Plan follow-up studies

---

## Budget

| Item | Cost | Notes |
|------|------|-------|
| Participant incentives | $1,200 | $50 × 24 participants |
| Transcription service | $240 | $10/hour × 24 hours |
| Video conferencing | $0 | Use existing Zoom |
| Analysis software | $0 | Use Notion/Sheets |
| Researcher time | Internal | ~120 hours (8 weeks × 15 hrs/week) |
| **Total cash cost** | **$1,440** | |

---

## Success Metrics for This Study

**Recruitment:**
- ✓ 24 interviews completed (100% target)
- ✓ ≥40% underrepresented genders
- ✓ ≥30% non-North America

**Data Quality:**
- ✓ ≥80% of interviews yield actionable insights
- ✓ ≥80% inter-rater reliability in coding
- ✓ Thematic saturation reached (no new themes after interview 20)

**Impact:**
- ✓ ≥5 actionable product recommendations
- ✓ Updated personas inform design decisions
- ✓ Improvement backlog prioritized and assigned

---

**Document Owner:** UX Researcher (Phase 4)
**Review Required By:** Behavioral Designer, Product Lead
**Status:** Ready for Execution
**Estimated Start Date:** Q1 2026
**Estimated Completion:** 8 weeks from start
